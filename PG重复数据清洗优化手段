# PG重复数据清洗优化手段

转https://github.com/digoal/blog/blob/master/201612/20161230_01.md

## 1. 窗口查询

主要用于筛选出重复值，并加上标记。

需要去重的字段作为窗口，规则字段作为排序字段，建立好复合索引，即可开始了。

## 2. 外部表

如果你的数据来自文本，那么可以采用一气呵成的方法来完成去重，即把数据库当成文本处理平台，通过PostgreSQL的file_fdw外部表直接访问文件，在SQL中进行去重。

## 3. 并行计算

如果你的数据来自文本，可以将文本切割成多个小文件，使用外部表，并行的去重，但是注意，去完重后，需要用merge sort再次去重。

另一方面，PostgreSQL 9.6已经支持单个QUERY使用多个CPU核来处理，可以线性的提升性能。（去重需要考虑合并的问题）。

## 4. 递归查询、递归收敛

使用递归查询，可以对重复度很高的场景进行优化，曾经在几个CASE中使用，优化效果非常明显，从几十倍到几百倍不等。

## 5. insert on conflict

PostgreSQL 9.5新增的特性，可以在数据导入时完成去重的操作。 直接导出结果。

## 6. LLVM

处理多行时，减少上下文切换。

性能可以提升一倍左右。

## 7. 流式计算

在数据导入过程中，流式去重，是不是很炫酷呢。

## 8. 并行创建索引

在创建索引时，为了防止堵塞DML操作，可以使用concurrently的方式创建，不会影响DML操作。

建立索引时，加大maintenance_work_mem可以提高创建索引的速度。

## 9. 并行读取文件片段导入

为了加快导入速度，可以切片，并行导入。

将来可以在file_fdw这种外部访问接口中做到分片并行导入。

## 10. bulk load, nologgin

如果数据库只做计算，也就是说在数据库中处理的中间结果无需保留时，可以适应bulk的方式导入，或者使用unlogged table。

可以提高导入的速度，同时导入时也可以关闭autovacuum.

## 小结

1. 如果数据已经在数据库中，在原表基础上，删除重复数据，耗时约2秒。

2. 如果数据要从文本导入，并将去重后的数据导出，整个流程约耗时5.28秒。
